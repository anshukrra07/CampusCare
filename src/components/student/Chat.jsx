import { useEffect, useState, useRef } from "react";
import { useNavigate } from "react-router-dom";
import { auth, db, storage } from "../../lib/firebase";
import { useAuthState } from "react-firebase-hooks/auth";
import {
  collection,
  addDoc,
  query,
  orderBy,
  onSnapshot,
  serverTimestamp,
  getDocs,
  updateDoc,
  limit,
} from "firebase/firestore";
import { ref, uploadBytes, getDownloadURL } from "firebase/storage";
import {
  PaperAirplaneIcon,
  ChatBubbleLeftRightIcon,
  UserIcon,
  SparklesIcon,
  MicrophoneIcon,
  StopIcon,
  XMarkIcon,
} from "@heroicons/react/24/outline";
import { useVoiceRecorder } from "../../hooks/useVoiceRecorder";
import VoiceMessage from "./VoiceMessage";

export default function Chat() {
  const [user] = useAuthState(auth);
  const navigate = useNavigate();
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState("");
  const [isTyping, setIsTyping] = useState(false);
  const [isSending, setIsSending] = useState(false);
  const [aiProcessing, setAiProcessing] = useState(false); // Track AI processing state
  const abortControllerRef = useRef(null); // For cancelling AI requests
  const messagesEndRef = useRef(null);
  const MESSAGE_LIMIT = 50; // Limit number of messages to load
  
  // Voice recording functionality
  const {
    isRecording,
    isPaused,
    recordingTime,
    audioBlob,
    isBlocked,
    startRecording,
    stopRecording,
    pauseRecording,
    resumeRecording,
    cancelRecording,
    formatTime,
    cleanup,
  } = useVoiceRecorder();

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  };
  
  // Execute AI-suggested UI actions
  const executeUIAction = (action) => {
    console.log('ðŸŽ¯ Executing AI UI action:', action);
    
    if (action.startsWith('navigate:')) {
      const path = action.replace('navigate:', '');
      console.log('ðŸ“ Navigating to:', path);
      navigate(path);
    } else if (action === 'show:mood-form') {
      // Could trigger a modal or redirect to mood tracking
      navigate('/mood-tracking');
    } else if (action === 'show:appointment-form') {
      navigate('/appointments');
    } else if (action === 'highlight:emergency') {
      // Could highlight emergency resources in UI
      navigate('/crisis-support');
    }
    // Add more UI actions as needed
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  // ðŸ”¥ Real-time listener for current user's chat with error handling
  useEffect(() => {
    if (!user || !auth.currentUser) {
      console.log('No authenticated user, skipping Firestore listener');
      return;
    }

    // Wait for authentication to be fully established
    if (!auth.currentUser.uid) {
      console.log('User UID not available yet, waiting...');
      return;
    }

    console.log('Setting up Firestore listener for user:', auth.currentUser.uid);

    const q = query(
      collection(db, "users", user.uid, "chats"),
      orderBy("created_at", "desc"),
      limit(MESSAGE_LIMIT)
    );

    const unsub = onSnapshot(
      q, 
      (snapshot) => {
        console.log(`Firestore snapshot: ${snapshot.docs.length}/${MESSAGE_LIMIT} messages loaded`);
        const docs = snapshot.docs.map((doc) => ({ id: doc.id, ...doc.data() }));
        
        // Filter out backend-generated transcribed text messages
        // These are created by processVoice and are redundant since the transcript 
        // is already shown in the VoiceMessage component
        const filteredDocs = docs.filter((doc) => {
          // Skip text messages that were generated by processVoice backend
          if (doc.type === 'text' && 
              doc.role === 'user' && 
              doc.meta?.processedBy === 'processVoice') {
            console.log('Filtering out backend-generated transcribed text message:', doc.text?.substring(0, 50) + '...');
            return false; // Hide this message
          }
          return true; // Keep all other messages
        });
        
        console.log(`Filtered ${docs.length - filteredDocs.length} transcribed text messages`);
        
        // Reverse since we're ordering by desc but want to display asc
        const reversedMessages = filteredDocs.reverse();
        setMessages(reversedMessages);

        // Handle typing indicator based on VISIBLE/FILTERED messages
        if (reversedMessages.length > 0) {
          const lastVisibleMsg = reversedMessages[reversedMessages.length - 1];
          const shouldShowTyping = lastVisibleMsg?.role === "user" && 
                                  !lastVisibleMsg.meta?.processedBy; // Show typing for all unprocessed user messages
          
          console.log('Typing indicator check:', {
            lastMessage: lastVisibleMsg?.text?.substring(0, 30),
            role: lastVisibleMsg?.role,
            processedBy: lastVisibleMsg?.meta?.processedBy,
            type: lastVisibleMsg?.type,
            shouldShow: shouldShowTyping
          });
          
          setIsTyping(shouldShowTyping);
          setAiProcessing(shouldShowTyping);
          
          // Check for AI UI actions in the last assistant message
          if (lastVisibleMsg?.role === "assistant" && lastVisibleMsg?.meta?.uiActions) {
            console.log('ðŸ¤– AI suggested UI actions:', lastVisibleMsg.meta.uiActions);
            // Execute UI actions after a short delay to let user see the response
            setTimeout(() => {
              lastVisibleMsg.meta.uiActions.forEach(action => {
                executeUIAction(action);
              });
            }, 1500); // 1.5 second delay
          }
        } else {
          setIsTyping(false);
          setAiProcessing(false);
        }
      },
      (error) => {
        console.error('Firestore connection error:', error);
        console.error('Error code:', error.code);
        console.error('Error message:', error.message);
        
        // Handle specific error codes
        if (error.code === 'permission-denied') {
          console.error('Permission denied - check Firestore security rules');
        } else if (error.code === 'unavailable') {
          console.error('Firestore service unavailable - will retry automatically');
        }
        
        // Reset loading states on error
        setIsTyping(false);
        setAiProcessing(false);
      }
    );

    return () => {
      console.log('Cleaning up Firestore listener');
      unsub();
    };
  }, [user, auth.currentUser]); // âœ… depend on user and auth state


  // âžœ Send text message
  const sendMessage = async (e) => {
    e.preventDefault();
    const trimmedInput = input.trim();
    if (!trimmedInput || !user || isTyping || isSending) return;

    // Verify authentication
    if (!auth.currentUser || !auth.currentUser.uid) {
      console.error('User not properly authenticated');
      alert('Authentication error. Please refresh and try again.');
      return;
    }

    setIsSending(true);
    try {
      console.log('Sending text message for user:', auth.currentUser.uid);
      await addDoc(collection(db, "users", user.uid, "chats"), {
        role: "user",
        type: "text",
        text: trimmedInput,
        created_at: serverTimestamp(),
      });
      setInput("");
      console.log('Text message sent successfully');
    } catch (error) {
      console.error("Error sending message:", error);
      if (error.code === 'permission-denied') {
        alert('Permission denied. Please check your account permissions.');
      } else {
        alert('Failed to send message. Please try again.');
      }
    } finally {
      setIsSending(false);
    }
  };

  // âž• Send voice message
  const sendVoiceMessage = async (audioBlob) => {
    if (!audioBlob || !user || isTyping || isSending) return;

    setIsSending(true);
    try {
      // Verify user is authenticated
      if (!auth.currentUser) {
        throw new Error('User not authenticated');
      }

      // Generate unique filename with webm extension
      const timestamp = Date.now();
      const filename = `voice_messages/${user.uid}/${timestamp}.webm`;
      const storageRef = ref(storage, filename);
      
      console.log('Uploading voice message:', filename);
      console.log('Blob size:', audioBlob.size, 'bytes');
      console.log('Blob type:', audioBlob.type);
      
      // Upload audio file to Firebase Storage
      const snapshot = await uploadBytes(storageRef, audioBlob);
      console.log('Upload successful:', snapshot.ref.fullPath);
      
      const audioUrl = await getDownloadURL(snapshot.ref);
      console.log('Download URL generated:', audioUrl);
      
      // Save voice message to Firestore (without processedBy so typing indicator shows)
      await addDoc(collection(db, "users", user.uid, "chats"), {
        role: "user",
        type: "voice",
        audioUrl,
        duration: recordingTime,
        created_at: serverTimestamp(),
        // Flag to prevent onChatMessage from processing this but allow typing indicator
        meta: { isVoiceMessage: true },
      });
      
      console.log('Voice message saved to Firestore');
      
      // Process voice with AI (transcription + emotion analysis)
      // Keep the AI thinking indicator on during processing
      try {
        await processVoiceWithAI(audioBlob, user.uid);
      } catch (aiError) {
        console.warn('Voice AI processing failed:', aiError);
        // Turn off indicators on error
        setIsTyping(false);
        setAiProcessing(false);
      }
      
    } catch (error) {
      console.error("Error sending voice message:", error);
      alert('Failed to send voice message. Please check your internet connection and try again.');
    } finally {
      setIsSending(false);
    }
  };

  // Stop AI processing
  const stopAI = () => {
    // Cancel ongoing AI request
    if (abortControllerRef.current) {
      abortControllerRef.current.abort();
      abortControllerRef.current = null;
    }
    
    // Reset states
    setIsTyping(false);
    setAiProcessing(false);
    
    console.log('AI processing stopped by user');
  };

  // Optional: Process voice with your Firebase Function
  const processVoiceWithAI = async (audioBlob, userId) => {
    // Create new abort controller for this request
    abortControllerRef.current = new AbortController();
    
    try {
      setAiProcessing(true);
      
      // Convert blob to base64
      const reader = new FileReader();
      const base64Audio = await new Promise((resolve, reject) => {
        reader.onload = () => {
          const result = reader.result.split(',')[1]; // Remove data:audio/webm;base64, prefix
          resolve(result);
        };
        reader.onerror = reject;
        reader.readAsDataURL(audioBlob);
      });

      // Call your processVoice Firebase Function via HTTP with abort signal
      const response = await fetch('https://processvoice-b7hiefsuya-el.a.run.app', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          userId,
          audioBase64: base64Audio,
          mimeType: audioBlob.type,
        }),
        signal: abortControllerRef.current.signal, // Enable request cancellation
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`HTTP error! status: ${response.status}, message: ${errorText}`);
      }

      const result = await response.json();
      console.log('Voice processing result:', result);
      
      // The function automatically saves transcription and AI response to Firestore
      // Now let's update the original voice message with transcript and emotion data
      if (result.transcript || result.emotion) {
        try {
          // Find and update the most recent voice message for this user
          const recentVoiceQuery = query(
            collection(db, "users", user.uid, "chats"),
            orderBy("created_at", "desc")
          );
          
          // Get the most recent message to update it with emotion data
          const snapshot = await getDocs(recentVoiceQuery);
          const messages = snapshot.docs;
          
          // Find the most recent voice message
          const recentVoiceMessage = messages.find(doc => {
            const data = doc.data();
            return data.type === "voice" && data.role === "user";
          });
          
          if (recentVoiceMessage) {
            await updateDoc(recentVoiceMessage.ref, {
              transcript: result.transcript,
              'meta.emotion': result.emotion,
              'meta.processedBy': 'voiceProcessingComplete' // Mark as fully processed
            });
            console.log('Updated voice message with transcript and emotion data');
          }
        } catch (updateError) {
          console.warn('Failed to update voice message with emotion data:', updateError);
        }
      }
      
      return result;
    } catch (error) {
      if (error.name === 'AbortError') {
        console.log('Voice AI processing cancelled by user');
        return null;
      }
      console.error('Error processing voice with AI:', error);
      throw error;
    } finally {
      setAiProcessing(false);
      abortControllerRef.current = null;
    }
  };

  // Handle voice recording actions
  const handleStartRecording = async () => {
    const success = await startRecording();
    if (!success && isBlocked) {
      alert("Microphone access is required for voice messages. Please enable microphone permissions.");
    }
  };

  const handleStopRecording = async () => {
    const blob = await stopRecording();
    if (blob) {
      // Show AI processing indicator immediately after voice recording
      setIsTyping(true);
      setAiProcessing(true);
      
      try {
        await sendVoiceMessage(blob);
      } finally {
        // The real-time listener will handle turning off the indicator
        // when the AI response comes back
      }
    }
  };

  const handleCancelRecording = () => {
    cancelRecording();
  };

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      cleanup();
      // Cancel any ongoing AI requests
      if (abortControllerRef.current) {
        abortControllerRef.current.abort();
      }
    };
  }, [cleanup]);

  // Show loading state when no user
  if (!user) {
    return (
      <div className="flex items-center justify-center h-full">
        <div className="text-center">
          <div className="animate-spin rounded-full h-12 w-12 border-b-2 border-indigo-500 mx-auto mb-4"></div>
          <p className="text-gray-600">Loading chat...</p>
        </div>
      </div>
    );
  }

  return (
    <div className="flex flex-col bg-white -m-6" style={{ height: 'calc(100vh - 80px)' }}>
      {/* Header - Fixed */}
      <div className="bg-gradient-to-r from-indigo-500 to-purple-600 px-6 py-4 shadow-lg flex-shrink-0">
        <div className="flex items-center justify-between">
          <div className="flex items-center">
            <div className="bg-white/20 p-2 rounded-full mr-3">
              <ChatBubbleLeftRightIcon className="h-6 w-6 text-white" />
            </div>
            <div>
              <h1 className="text-xl font-semibold text-white">CampusCare AI Counselor</h1>
              <p className="text-indigo-100 text-sm">Here to support your mental wellness journey</p>
            </div>
          </div>
          
        </div>
      </div>

      {/* Messages Container - Scrollable */}
      <div className="flex-1 overflow-y-auto bg-gray-50 min-h-0">
        {messages.length === 0 ? (
          <div className="flex items-center justify-center h-full">
            <div className="text-center max-w-md mx-auto px-6">
              <div className="bg-indigo-100 p-4 rounded-full w-20 h-20 flex items-center justify-center mx-auto mb-4">
                <SparklesIcon className="h-10 w-10 text-indigo-600" />
              </div>
              <h3 className="text-lg font-medium text-gray-900 mb-2">Welcome to your AI counselor!</h3>
              <p className="text-gray-600 text-sm">
                I'm here to provide support and guidance for your mental wellness. Feel free to share what's on your mind.
              </p>
            </div>
          </div>
        ) : (
          <div className="px-4 py-6 space-y-6">
            {messages.map((msg) => (
              <MessageBubble key={msg.id} message={msg} />
            ))}
            
            {/* Typing indicator with Stop button */}
            {isTyping && (
              <div className="flex items-start space-x-3">
                <div className="bg-indigo-500 p-2 rounded-full">
                  <SparklesIcon className="h-4 w-4 text-white" />
                </div>
                <div className="bg-white rounded-2xl rounded-tl-sm px-4 py-3 shadow-sm border max-w-xs">
                  <div className="flex items-center justify-between">
                    <div className="flex items-center space-x-2">
                      <div className="flex space-x-1">
                        <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" style={{ animationDelay: '0ms' }}></div>
                        <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" style={{ animationDelay: '150ms' }}></div>
                        <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" style={{ animationDelay: '300ms' }}></div>
                      </div>
                      <p className="text-xs text-gray-500">AI is thinking...</p>
                    </div>
                    
                    {/* Stop AI Button */}
                    {aiProcessing && (
                      <button
                        onClick={stopAI}
                        className="ml-3 p-1.5 bg-red-100 hover:bg-red-200 text-red-600 rounded-full transition-colors duration-200 flex-shrink-0"
                        title="Stop AI response"
                      >
                        <StopIcon className="h-3 w-3" />
                      </button>
                    )}
                  </div>
                </div>
              </div>
            )}
            <div ref={messagesEndRef} />
          </div>
        )}
      </div>

      {/* Input Area - Fixed */}
      <div className="border-t border-gray-200 bg-white px-4 py-4 flex-shrink-0">
        {/* Voice Recording UI */}
        {isRecording && (
          <div className="mb-4 p-4 bg-red-50 border border-red-200 rounded-2xl">
            <div className="flex items-center justify-between">
              <div className="flex items-center space-x-3">
                <div className="bg-red-500 p-2 rounded-full animate-pulse">
                  <MicrophoneIcon className="h-4 w-4 text-white" />
                </div>
                <div>
                  <p className="text-sm font-medium text-red-900">
                    {isPaused ? 'Paused' : 'Recording...'}
                  </p>
                  <p className="text-xs text-red-600">
                    {formatTime(recordingTime)}
                  </p>
                </div>
              </div>
              
              <div className="flex items-center space-x-2">
                {isPaused ? (
                  <button
                    onClick={resumeRecording}
                    className="bg-red-500 hover:bg-red-600 text-white p-2 rounded-full transition-colors"
                  >
                    <MicrophoneIcon className="h-4 w-4" />
                  </button>
                ) : (
                  <button
                    onClick={pauseRecording}
                    className="bg-yellow-500 hover:bg-yellow-600 text-white p-2 rounded-full transition-colors"
                  >
                    <StopIcon className="h-4 w-4" />
                  </button>
                )}
                
                <button
                  onClick={handleStopRecording}
                  disabled={isSending}
                  className="bg-green-500 hover:bg-green-600 disabled:bg-gray-300 text-white p-2 rounded-full transition-colors"
                >
                  <PaperAirplaneIcon className="h-4 w-4" />
                </button>
                
                <button
                  onClick={handleCancelRecording}
                  className="bg-gray-500 hover:bg-gray-600 text-white p-2 rounded-full transition-colors"
                >
                  <XMarkIcon className="h-4 w-4" />
                </button>
              </div>
            </div>
          </div>
        )}

        {/* Regular Input Form */}
        {!isRecording && (
          <form onSubmit={sendMessage} className="flex items-end space-x-3">
            <div className="flex-1">
              <div className="relative">
                <input
                  value={input}
                  onChange={(e) => setInput(e.target.value)}
                  placeholder="Share what's on your mind..."
                  className="w-full px-4 py-3 border border-gray-300 rounded-2xl focus:ring-2 focus:ring-indigo-500 focus:border-transparent resize-none bg-gray-50 focus:bg-white transition-colors"
                  disabled={!user || isSending}
                />
              </div>
            </div>
            
            {/* Voice Record Button */}
            <button
              type="button"
              onClick={handleStartRecording}
              disabled={!user || isTyping || isSending || isBlocked}
              className="bg-gray-100 hover:bg-gray-200 disabled:bg-gray-300 text-gray-600 p-3 rounded-full transition-colors duration-200 shadow-lg hover:shadow-xl disabled:cursor-not-allowed"
              title="Record voice message"
            >
              <MicrophoneIcon className="h-5 w-5" />
            </button>
            
            {/* Send Text Button */}
            <button
              type="submit"
              disabled={!input.trim() || !user || isTyping || isSending}
              className="bg-indigo-500 hover:bg-indigo-600 disabled:bg-gray-300 text-white p-3 rounded-full transition-colors duration-200 shadow-lg hover:shadow-xl disabled:cursor-not-allowed"
            >
              <PaperAirplaneIcon className="h-5 w-5" />
            </button>
          </form>
        )}
        
        <p className="text-xs text-gray-500 mt-2 text-center">
          {isBlocked 
            ? "Microphone access required for voice messages. Please enable in your browser settings."
            : "Your conversations are private and secure. Press Enter to send or hold mic to record."
          }
        </p>
      </div>
    </div>
  );
}

function MessageBubble({ message }) {
  const isUser = message.role === "user";
  const isHighRisk = message.meta?.risk === "high";
  const isVoiceMessage = message.type === "voice";
  
  return (
    <div className={`flex items-start space-x-3 ${isUser ? 'flex-row-reverse space-x-reverse' : ''}`}>
      {/* Avatar */}
      <div className={`p-2 rounded-full ${
        isUser 
          ? 'bg-indigo-500' 
          : isHighRisk 
          ? 'bg-red-500' 
          : 'bg-indigo-500'
      }`}>
        {isUser ? (
          <UserIcon className="h-4 w-4 text-white" />
        ) : (
          <SparklesIcon className="h-4 w-4 text-white" />
        )}
      </div>
      
      {/* Message */}
      <div className={`max-w-xs lg:max-w-md ${
        isUser ? 'text-right' : 'text-left'
      }`}>
        {/* Voice Message */}
        {isVoiceMessage ? (
          <VoiceMessage 
            audioUrl={message.audioUrl}
            duration={message.duration}
            isUser={isUser}
            timestamp={message.created_at}
            transcript={message.transcript}
            emotion={message.meta?.emotion}
          />
        ) : (
          /* Text Message */
          <div>
            <div className={`rounded-2xl px-4 py-3 shadow-sm border ${
              isUser 
                ? 'bg-indigo-500 text-white rounded-br-sm'
                : isHighRisk
                ? 'bg-red-50 border-red-200 rounded-tl-sm'
                : 'bg-white rounded-tl-sm'
            }`}>
              <p className={`text-sm ${
                isUser 
                  ? 'text-white' 
                  : isHighRisk 
                  ? 'text-red-900' 
                  : 'text-gray-900'
              }`}>
                {message.text}
              </p>
            </div>
            
            {/* Emotion Display for Text Messages (same style as voice) */}
            {message.meta?.emotion && (
              <div className={`mt-2 text-xs p-2 rounded-lg ${
                isUser 
                  ? 'bg-indigo-100 text-indigo-800'
                  : 'bg-gray-100 text-gray-600'
              }`}>
                <div className="opacity-75">
                  Emotion: {message.meta.emotion.emotion} ({message.meta.emotion.intensity}%)
                </div>
              </div>
            )}
          </div>
        )}
        
        {/* Timestamp */}
        {message.created_at?.seconds && (
          <p className={`text-xs text-gray-500 mt-1 ${
            isUser ? 'text-right' : 'text-left'
          }`}>
            {new Date(message.created_at.seconds * 1000).toLocaleTimeString(
              [],
              { hour: "2-digit", minute: "2-digit" }
            )}
          </p>
        )}
        
        {/* Model Information */}
        {!isUser && message.meta && (
          <div className="mt-1 text-left">
            <div className="flex flex-wrap gap-1">
              {message.meta.responseModel && (
                <span className="inline-flex items-center px-2 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-700">
                  ðŸ¤– {message.meta.responseModel}
                </span>
              )}
              {message.meta.emotionModel && (
                <span className="inline-flex items-center px-2 py-0.5 rounded-full text-xs font-medium bg-purple-100 text-purple-700">
                  ðŸ’­ {message.meta.emotionModel}
                </span>
              )}
              {message.meta.tts && (
                <span className="inline-flex items-center px-2 py-0.5 rounded-full text-xs font-medium bg-green-100 text-green-700">
                  ðŸ”Š TTS
                </span>
              )}
            </div>
          </div>
        )}
        
        {/* UI Actions indicator */}
        {!isUser && message.meta?.uiActions && message.meta.uiActions.length > 0 && (
          <div className="mt-2 text-left">
            <div className="flex flex-wrap gap-1">
              {message.meta.uiActions.map((action, index) => {
                const actionText = action.startsWith('navigate:') 
                  ? `Navigating to ${action.replace('navigate:', '').replace('/', '').replace('-', ' ')}`
                  : action.replace('show:', 'Showing ').replace('highlight:', 'Highlighting ');
                  
                return (
                  <span key={index} className="inline-flex items-center px-2 py-1 rounded-full text-xs font-medium bg-green-100 text-green-800">
                    ðŸŽ¯ {actionText}
                  </span>
                );
              })}
            </div>
          </div>
        )}
        
        {/* Crisis indicator */}
        {isHighRisk && (
          <div className="mt-2 text-left">
            <span className="inline-flex items-center px-2 py-1 rounded-full text-xs font-medium bg-red-100 text-red-800">
              Crisis support activated
            </span>
          </div>
        )}
      </div>
    </div>
  );
}